# Linearization

```nocode, noout
using Plots
backend(:gadfly)
```

The derivative of $f(x)$ has the interpretation as the slope of the
tangent line. The tangent line is the line that best approximates the
function at the point.

Using the point-slope form of a line, we see that the tangent line to the graph of $f(x)$ at $(c,f(c))$ is given by:

$$~
y = f(c) + f'(c) \cdot (x - c).
~$$

This is written as an equation, though we prefer to work with
functions within `Julia`. Here we write such a function as an
operator -- it takes a function `f` and returns a function
representing the tangent line.

```
using Roots
tl(f, c) = x -> f(c) + D(f)(c) * (x - c)
```

(Recall, the `->` indicates that an anonymous function is being generated.)


To illustrate the tangent line, we can make some graphs:

```
using Plots
f(x) = x^2
plot([f, tl(f, -1), tl(f, 2)], -3, 3)
```

The graph shows that near the point, the line and function are close,
but this need not be the case away from the point. We can express this informally as

$$~
f(x) \approx f(c) + f'(c) \cdot (x-c)
~$$

with the understanding this applies for $x$ "close" to $c$.

This section gives some implications of this fact and quantifies what
"close" can mean.

##### Example

There are several approximations that are well known in physics:

* $\sin(x) \approx x$ at $x=0$:

```
f(x) = sin(x)
plot([f, tl(f, 0)], -pi/2, pi/2)
```

* $\log(1 + x) \approx x$ at $x=0$:

```
f(x) = log(1 + x)
plot([f, tl(f, 0)], -1/2, 1/2)
```

* $1/(1-x) \approx x$ at $x=0$:

```
f(x) = 1/(1-x)
plot([f, tl(f, 0)], -1/2, 1/2)
```

In each of these three cases, a more complicated non-linear function
is well approximated in a region of interest by a simple linear
function.

## Numeric approximations

```
f(x) = sin(x)
xs =linspace(-1/4, pi/2)
p = Gadfly.plot(
      Gadfly.layer(x=xs, y=map(f, xs), Gadfly.Geom.line),
      Gadfly.layer(x=xs, y=xs, Gadfly.Geom.line),
      Gadfly.layer(x=[0,1,1], y = [0, 0, 1], Gadfly.Geom.line, Gadfly.Theme(default_color=Gadfly.color("brown"))),
      Gadfly.layer(x=[0,1,1], y = [0, 0, sin(1)], Gadfly.Geom.line, Gadfly.Theme(default_color=Gadfly.color("gray"))),
      Gadfly.layer(x=[1/2,1, 1/2-1/8],  y=[0, sin(1)/2, 1], label=["Δx", "Δy", "m=dy/dx"], Gadfly.Geom.label)
      )
Outputonly(p)	  
```

The plot shows the tangent line with slope $dy/dx$ and the actual
change in $y$, $\Delta y$, for some specified $\Delta x$. The small
gap above the sine curve is the error. We can see that approximating
the value of $\Delta y = \sin(c+\Delta x) - \sin(c)$ with the often
easier to compute $f'(c)\Delta x$ is -- for small enough values of
$\Delta x$ -- not going to be too far off.

This approximation is known as linearization. It can be used both in
theoretical computations, and in pratical applications. To see how
effective it is, we look at some examples:

- $f(x) = \sin(x)$, $c=0$ and $\Delta x= 0.1$. Then the values are:

```
f(x) = sin(x)
c, deltax = 0, 0.1
f(c + deltax) - f(c), D(f)(c) * deltax
```

The values are pretty close. But what is $0.1$ radians? Lets use degrees. Suppose we have $\Delta x = 10^\circ$:

```
c, deltax = 0, 10*pi/180
actual=f(c + deltax) - f(c)
approx = D(f)(c) * deltax
actual, approx
```

They agree until the third decimal value. The *percentage error* is just $1/2$ a percent:

```
(approx - actual) / actual * 100
```


##### Example

We are traveling 60 miles. At 60 miles an hour, we will take 60 minutes (or one hour). How long will it take at 70 miles an hour? (Assume you can't divide, but only multiply!)


Well the answer is $60/70$ hours or $60/70 \cdot 60$ minutes. But we
can't divide, so we turn this into a multiplication problem via some algebra:

$$~
\frac{60}{70} = \frac{60}{60 + 10} = \frac{1}{1 + 10/60} = \frac{1}{1 + 1/6}.
~$$

Okay, so far no calculator was needed. We wrote $70 = 60 + 10$, as we
know that $60/60$ is just $1$. This almost gets us there. If we really
don't want to divide, we can get an answer by using the tangent line
approximation for $1/(1+x)$ around $x=0$. This is $1/(1+x) \approx 1 -
x$. (You can check by finding that $f'(0) = -1$.) Thus, our answer is
approximately $5/6$ of an hour or 50 minutes.

How much in error are we?

```
abs(50 - 60/70*60) / (60/70*60) * 100
```

That's about $3$ percent. Not bad considering we could have done all the above in our head.



###### Example from physics

A *simple* pendulum is comprised of a massless "bob" on a rigid "rod"
of length $l$. The rod swings back and forth making an angle $\theta$
with the perpendicular. At rest $\theta=0$, swinging $\lvert\theta\rvert \leq \theta_0$
for some $\theta_0$.

According to [Wikipedia](http://tinyurl.com/yz5sz7e) -- and many
introductory physics book -- while swinging the angle $\theta$ varies
with time following this equation:

$$~
\theta''(t) + \frac{g}{l} \sin(\theta(t)) = 0.
~$$

That is, the second derivative of $\theta$ is proportional to the sine
of $\theta$ where the proportionality constant involves $g$ from
gravity and the length of the "rod."

This would be much easier if the second derivative were proportional to the angle $\theta$ and not its sine.

[Huygens](http://en.wikipedia.org/wiki/Christiaan_Huygens) used the
approximation of $\sin(x) \approx x$, noted above, to say that when
the angle is not too big, we have the pendulum's swing obeying
$\theta''(t) = -g/l \cdot t$. Without getting too involved in why,
we can verify that $\theta_0\sin(\sqrt{g/l}\cdot t)$ will be a solution to this
equation.

This says the motion is periodic with constant amplitude (assuming frictionless behaviour), as
the sine function is. More surprisingly, the period is found from $T =
2\pi/(\sqrt{g/l}) = 2\pi \sqrt{l/g}$. It depends on $l$ -- longer
"rods" take more time to swing back and forth -- but does not depend
on the how wide the pendulum is swinging between (provided $\theta_0$
is not so big the approximation of $\sin(x) \approx x$ fails). This
latter fact may be surprising, though not to Galileo who discovered
it.




## The actual error

How good is the approximation? Graphically we can see it is pretty
good for the graphs we choose, but are there graphs out there for
which the approximation is not so good?  Of course. However, we can
say this (the
[Lagrange](http://en.wikipedia.org/wiki/Taylor%27s_theorem) form of a
more general Taylor remainder theorem):

> Let $f(x)$ be twice differentiable on $I=(a,b)$, and $a < c <
> b$. Then for any $x$ in $I$, there exists some value $\xi$ between $c$  and $x$ such that
> $$~f(x) = f(c) + f'(c)(x-c) + f''(\xi)\cdot(x-c)^2/2.~$$

That is, the error is basically a constant times a quadratic function
centered at $c$. 

For $\sin(x)$ at $c=0$ we get $\lvert\sin(x) - x\rvert = \lvert-\sin(\xi)\cdot x^2/2\rvert$.
Since $\lvert\sin(\xi)\rvert \leq 1$, we must have this bound:
$\lvert\sin(x) - x\rvert \leq x^2/2$.


Can we verify? Let's do so graphically:

```
h(x) = abs(sin(x) - x)
g(x) = x^2/2
plot([h,g], -2, 2)
```


Similarly, for $f(x) = \log(1 + x)$ we have the following at $c=0$:

$$~
f'(x) = 1/(1+x), \quad f''(x) = -1/(1+x)^2.
~$$
 
So, as $f(c)=0$ and $f'(c) = 1$, we have

$$~
\lvert f(x) - x\rvert \leq \lvert f''(\xi)\rvert \cdot \frac{x^2}{2}
~$$

We see that $\lvert f''(x)\rvert$ is decreasing for $x > -1$. So if $-1 < x < c$ we have

$$~
\lvert f(x) - x\rvert \leq \lvert f''(x)\rvert \cdot \frac{x^2}{2} = \frac{x^2}{2(1+x)^2}.
~$$

And for  $c=0 < x$, we have

$$~
\lvert f(x) - x\rvert \leq \lvert f''(0)\rvert \cdot \frac{x^2}{2} = x^2/2.
~$$


Plotting we verify:

```
h(x) = abs(log(1+x) - x)
g(x) = x < 0 ? x^2/(2*(1+x)^2) : x^2/2
plot([h,g], -.5, 2)
```

### Why is the remainder term as it is?

To see formally why the remainder is as it is uses the mean value
theorem in the extended form of Cauchy. Suppose $c=0$ and let $h(x) = f(x) - (f(0) +
f'(0) x)$ and $g(x) = x^2$. Then we have that there exists a $e$ with
$0 < e < x$ such that

$$~
\text{error} = h(x) - h(0) = (g(x) - g(0)) \frac{h'(x)}{g'(x)} = x^2 \cdot \frac{1}{2} \cdot \frac{f'(e) - f'(0)}{e} =
x^2 \cdot \frac{1}{2} \cdot f''(\xi).
~$$

The value of $\xi$, from the mean value theorem applied to $f'(x)$,
satisfies $0 < \xi < e < x$, so is in $[0,x]$.

### The big (and small) "oh"

`SymPy` can find the tangent line expression as a special case of its `series` function (which implements Taylor series). Here we see the answer provided for $e^{\sin(x)}$:

```
using SymPy
x = symbols("x", real=true)
series(exp(sin(x)),x,0, 2)
```

We see the expression $1 + x$ which comes from the fact that `exp(sin(0))` is $1$, and the derivative `exp(sin(0)) * cos(0)` is *also* $1$. But what is the $\mathcal{O}(x^2)$?

We know the answer is *precisely* $f''(\xi)/2 \cdot x^2$, but were we
only concerned about the scale as $x$ goes to zero we would say this
is some well-behaved value (converging to $1 - f''(0)/2$) times
$x^2$. The [big](http://en.wikipedia.org/wiki/Big_O_notation) "oh"
notation says just that: were we take this term and divide by $x^2$
the limit --if it exists -- would be bounded. A little "oh" (e.g., $\mathcal{o}(x^2)$) would mean
that limit would be $0$.

Big "oh" and little "oh" give us a sense of how good an approximation
is without being bogged down in the details of the exact value. As
such they are useful guides in focusing on what is primary and what is
secondary. Applying this to our case, we have this rough form of the
tangent line approximation valid for functions having a second
derivative:

$$~
f(x) = f(c) + f'(c)(x-x) + \mathcal{O}(x^2).
~$$

##### Example

The `D` operator of the `Roots` package for first derivatives does
nothing more than work with the tangent line expression. What is
surprising is that if all we care about is the information contained
in the tangent line expressions, then we can combine functions by
adding, multiplying, dividing, and composing using just their tangent
line expressions and not lose any information.



Consider $\sin(x)$ and $\exp(x)$ written in terms of their tangent
lines about $c=0$:

```
Sin = series(sin(x), x, 0, 2)
Exp = series(exp(x), x, 0, 2)
Sin, Exp
```

A series for their product (at $0$) is:

```
series(sin(x) * cos(x), x, 0, 2)
```

Which is the same as the product of their series (at $0$):

```
Sin * Exp |> simplify
```

This can be seen as some constant times $\mathcal{O}(x^2)$ will still be
$\mathcal{O}(x^2)$, as will any term containing a power of $x$. This
greatly simplifies the expression:

$$~
(x + \mathcal{O}(x^2)) \cdot (1 + x + \mathcal{O}(x^2)) =
x\cdot(1+x) + x \cdot  \mathcal{O}(x^2) + (1+x) \cdot \mathcal{O}(x^2) =
x + x^2 +  x \cdot  \mathcal{O}(x^2) + (1+x) \cdot \mathcal{O}(x^2) =
x + \mathcal{O}(x^2).
~$$

The last equals sign might be surprising, but follows from properties
of limits: $x^2$ is clearly big "oh" of $x^2$, and a constant of
positive power of $x$ times $\mathcal{O}(x^2)$ will still be
$\mathcal{O}(x^2)$. This "algebra" makes things easy: multiply the linear terms, then drop any $x^2$ parts.



To see another one, lets predict what the series would be for $\sin(x)^2$, using our "simplified" algebra rules:

$$~
(x + \mathcal{O}(x^2)) \cdot (x + \mathcal{O}(x^2)) = (x \cdot x) + \mathcal{O}(x^2) = \mathcal{O}(x^2).
~$$

That is, the linear approximation is the flat line $y=0$ near $0$ for $\sin(x)^2$.

We can check this with:

```
series(sin(x)^2, x, 0, 2)
```

(Basically, as $\sin(x) \approx x$ at $0$, $\sin(x)^2 \approx x^2$ which is $\mathcal{O}(x^2)$.)


In general, we have the tangent line expansion about $c=0$ for the product of functions uses the following formula involving the tangent line approximation (at $c=0$) of each individual function:

$$~
(a + bx + \mathcal{O}(x^2)) \cdot (c + dx + \mathcal{O}(x^2)) = (a + bx)\cdot(c + dx) + \mathcal{O}(x^2) =
ac + (ad + bc)x + bdx^2 + \mathcal{O}(x^2) =
ac + (ad + bc)x + \mathcal{O}(x^2).
~$$


What about composition? The expression $\exp(\sin(x))$ around $0$
would use the expansion of $\exp(x)$ at $\sin(0)$ and the expansion of
$\sin(x)$ around $0$. In this case, both are around $0$ (though this
need not be true in general). So we basically just substitute $x$ into
$1 + x$ and keep the big "oh" term: that is, we should have this
expression is no more than $1 + (x) + \mathcal{O}(x^2)$:

```
series(exp(sin(x)),x, 0, 2)
```

That is we just compose the two tangent line expressions -- provided they are taken about the correct values of $c$.

Note that $\sin(\exp(x))$ will not have the series $(1+x) +
\mathcal{O}(x^2)$, which comes from composing $x + \mathcal{O}(x^2)$
with $1 + x + \mathcal{O}(x^2)$ as the first one is *not* the
expansion of $\sin(x)$ about $\exp(0)$. Here we verify:

```
series(sin(exp(x)),x, 0, 2)
```

Finally, what about quotients? We can use the fact that $1/(c+d\cdot x) = 1/c
-d/c^2 x + \mathcal{O}(x^2)$ to see that if $c \neq 0$ we have in general:

$$~
\frac{a + bx +  \mathcal{O}(x^2)}{c + dx +  \mathcal{O}(x^2)} = (a + bx) \cdot 1/c^2(c - d \cdot x) + \mathcal{O}(x^2)
= a/c + (bc - ad)/c^2 \cdot x + \mathcal{O}(x^2).
~$$

In our specific case, lets verify for $\sin(x)/\exp(x)$:

```
series(sin(x)/exp(x), x, 0, 2)
```

And compare with this:

```
a,b = 0,1 # sin
c,d = 1,1 # exp
(a/c, (b*c - a*d)/c^2)
```

We recover the two coefficients, as expected.



Can we get the information about the derivatives of combinations of
functions from just the tangent line information of each function?
The derivative at $f$ at $c$ is just the $x$ coefficient in the
tangent line expansion at $c$ (recall we have $f(c) + f'(c)
\cdot(x-c) + \mathcal{O}((x-c)^2))$. So it seems likely, but let's check.



From the general expansions seen earlier for products and quotients,
we see the product rule in $ad + bc$ and the quotient rule in the
$(bc - ad)/c^2$. We can test with:

```
diff(exp(x) * sin(x), x) |> subs(x, 0)
```

and compare to a value computed just from the tangent line approximations

```
diff(Exp) * Sin + Exp * diff(Sin) |> simplify
```

As well, 

```
diff(sin(x)/exp(x), x) |> subs(x, 0)
```

Can be recovered from the approximations via the quotient rule. (We
use that $1 / (1+x+\mathcal{O}(x^2))^2 = 1 - x +\mathcal{O}(x^2)$ by
the work above.)

```
(diff(Sin) * Exp - Sin * diff(Exp)) * (1 - x) |> simplify
```

Finally for this example, the chain rule applied to $\exp(\sin(x))$ shows a derivative of $1$ at $0$:

```
diff(exp(sin(x)), x) |> subs(x, 0)
```

But from our expansion around $0$ for $\exp(\sin(x)) = 1 + x +
\mathcal{O}(x^2)$, we can read off from the $x$ coefficient that this
has derivative $1$ at $0$.





## Questions

###### Question

What is the right linear approximation for $\sqrt{1 + x}$ near $0$?

```
choices = [
L"1 + 1/2",
L"1 + x^{1/2}",
L"1 + (1/2)\cdot x",
L"1 - (1/2) \cdot x"]
ans = 3
radioq(choices, ans)
```


###### Question


What is the right linear approximation for $(1 + x)^k$ near $0$?

```
choices = [
L"1 + k",
L"1 + x^k",
L"1 + k\cdot x",
L"1 - k \cdot x"]
ans = 3
radioq(choices, ans)
```

###### Question

What is the right linear approximation for $\cos(\sin(x))$ near $0$?

```
choices = [
L"1",
L"1 + x",
L"x",
L"1 - x^2/2"
]
ans = 1
radioq(choices, ans)
```


###### Question

What is the  right linear approximation for $\tan(x)$ near $0$?

```
choices = [
L"1",
L"x",
L"1 + x",
L"1 - x"
]
ans = 2
radioq(choices, ans)
```



###### Question

What is the right linear approximation of $\sqrt{25 + x}$ near $x=0$?

```
choices = [
L"5 \cdot (1 + (1/2) \cdot (x/25))",
L"1 - (1/2) \cdot x",
L"1 + x",
L"25"
]
ans = 1
radioq(choices, ans)
```



###### Question


Let $f(x) = \sqrt{x}$. Find the actual error in approximating $f(26)$ by the
value of the tangent line at $(25, f(25))$ at $x=26$.

```
tl(x) = 5 + x/10
ans = tl(1) - sqrt(26)
numericq(ans)
```

###### Question

An estimate of some quantity was $12.34$ the actual value was $12$. What was the *percentage error*?

```
est = 12.34
act = 12.0
ans = (est -act)/act * 100
numericq(ans)
```


###### Question

Find the percentage error in estimating $\sin(5^\circ)$ by $5 \pi/180$.

```
tl(x) = x
x0 = 5 * pi/180
est = x0
act = sin(x0)
ans = (est -act)/act * 100
numericq(ans)
```

###### Question

The side length of a square is measured roughly to be $2.0$ cm. The actual length $2.2$ cm. What is the difference in area (in absolute values) as *estimated* by a tangent line approximation.

```
tl(x) = 4 + 4x
ans = tl(.2) - 4
numericq(abs(ans))
```

