# Applications with scalar functions



```
using Plots, LinearAlgebra, ForwardDiff, SymPy

D(f, n=1) = n > 1 ? D(D(f), n-1) : x -> ForwardDiff.derivative(f, float(x))
Base.adjoint(f::Function) = D(f)   # add f' notation for functions
grad(f) = (x, xs...) -> ForwardDiff.gradient(f, vcat(x, xs...))

unzip(vs) = Tuple(eltype(first(vs))[xyz[j] for xyz in vs] for j in eachindex(first(vs)))
unzip(r::Function, a, b, n=100) = unzip(r.(range(a, stop=b, length=n)))

function arrow!(plt::Plots.Plot, p, v; kwargs...)
  length(p) == 2 && return quiver!(plt, unzip([p])..., quiver=Tuple(unzip([v])); kwargs...)
  length(p) == 3 && return plot!(plt, unzip([p, p+v])...; kwargs...)
end
arrow!(p, v; kwargs...) = arrow!(Plots.current(), p, v; kwargs...)
```


## Tangent planes, linearization


Consider the case $f:R^2 \rightarrow R$. We visualize $z=f(x,y)$ through a surface. At a point $(a, b)$, this surface, if $f$ is sufficiently smooth, can be approximated by a flat area, or a plane. For example, the Northern hemisphere of the earth, might be modeled simplisitically by $z = \sqrt{R^2 - (x^2 + y^2)}$ for some $R$ and with the origin at the earth's core. The ancient view of a "flat earth," can be more generously seen as identifying this tangent plane with the sphere. More apt for current times, is the use of GPS coordinates to describe location. The difference between any two coordinates is technically a distance on a curved, nearly spherical, surface. But if the two points are reasonably closes (miles, not tens of miles) and accuracy isn't of utmost importance (i.e., not used for self-driving cars), then the distance can be found from the Euclidean distance formula, $\sqrt{(\Delta\text{latitude})^2 + \Delta\text{longitude})^2}$. That is, as if the points were on a plane, not a curved surface.

For the univariate case, the tangent line has many different uses. Here we see the tangent plane also does.


### Equation of the tangent plane

The partial derivatives have the geometric view of being the derivative of the univariate functions $f(\vec\gamma_x(t))$ and $f(\vec\gamma_y(t))$, where $\vec\gamma_x$ moves just parallel to the $x$ axis and $\vec\gamma_y$ moves just parallel to the $y$ axis. The partial derivatives then are slopes of tangent lines to each curve. The tangent plane, should it exist, should match both slopes at a given point. With this observation, we can identify it.

Consider $f(\vec\gamma_x)$ at a point $(a,b)$. The path has a tangent vector, which has "slope" $\frac{\partial f}{\partial x}$. and in the direction of the $x$ axis, but not the $y$ axis, as does this vector:  $\langle 1, 0, \frac{\partial f}{\partial x} \rangle$. Simlarly, this vector $\langle 0, 1, \frac{\partial f}{\partial y} \rangle$ describes the tangent line to $f(\vec\gamma_y)$ a the point.

```nocode
f(x,y) = 6 - x^2 -y^2
f(x)= f(x...)

a,b = 1, -1/2
γ_x(t) = [a, b] + t*[1,0]
γ_y(t) = [a, b] + t*[0,1]
tplane(x,y) = f(a,b) + grad(f)(a,b) ⋅ [x-a, y-b]

# draw surface
xr = 7/4
xs = ys = range(-xr, xr, length=100)
surface(xs, ys, f, legend=false)

# plot paths in x and y direction through (a,b)
plot!(unzip(t -> [γ_x(t)..., (f∘γ_x)(t)], -xr-a, xr-a)..., linewidth=3)
plot!(unzip(t -> [γ_y(t)..., (f∘γ_y)(t)], -xr-b, xr-b)..., linewidth=3)

# draw directional derivatives in 3d and normal
pt = [a, b, f(a,b)]
fx, fy = grad(f)(a,b)
arrow!(pt, [1, 0, fx], linewidth=3)
arrow!(pt, [0, 1, fy], linewidth=3)
arrow!(pt, [-fx, -fy, 1], linewidth=3) # normal

# draw point in base, x-y, plane
pt = [a, b, 0]
scatter!(unzip([pt])...)
arrow!(pt, [1,0,0], linestyle=:dash)
arrow!(pt, [0,1,0], linestyle=:dash)

# represent plane as 3d polygon
pts = [(a-1,b-1), (a+1, b-1), (a+1, b+1), (a-1, b+1), (a-1, b-1)]
plot!(unzip([[pt..., tplane(pt...)] for pt in pts])...)
```

These two vectors will lie in the plane. The normal vector is found by their cross product:

```
using SymPy
@vars f_x f_y
n = [1, 0, f_x] × [0, 1, f_y]
```

Let $\vec{x} = \langle a, b, f(a,b)$. The tangent plane at $\vec{x}$ then is described by all vectors $\vec{v}$ with $\vec{n}\cdot(\vec{v} - \vec{x})  = 0$. Using $\vec{v} = \langle u,v,w\rangle$, we have:

$$~
[-\frac{\partial f}{\partial x}, -\frac{\partial f}{\partial y}, 1] \cdot [x-a, y-b, z - f(a,b)] = 0,
~$$

or,

$$~
z = f(a,b) + \frac{\partial f}{\partial x} (x-a) + \frac{\partial f}{\partial y} (y-b),
~$$

which is more compactly expressed as

$$~
z = f(a,b) + \nabla(f) \cdot \langle x-a, y-b \rangle.
~$$

This form would then generalize to scalar functions from $R^n \rightarrow R$. This is consistent with the definition of $f$ being differentiable, where $\grad{f}$ plays the role of the slope in the formulas.


#### Alternate forms

The equation for the tangent plane is often expressed in a more explicit form. For $n=2$, if we set $dx = x-a$ and $dy=y-a$, then the equation for the plane becomes:

$$~
f(a,b) + \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy,
~$$

which is a common form for the equation, though possibly confusing, as $\partial x$ and $dx$ need to be distinguished. For $n > 2$, additional terms follow this pattern. This explicit form is helpful when doing calculations by hand, but much less so when working on the computer, say with `Julia`, as the representations using vectors (or matrices) can be readily implemented and their representation much closer to the formulas. For example, consider these two possible functions to find the tangent plane (returned as a function) at a point in 2 dimensions

```
function tangent_plane(f, pt)
fx, fy = ForwardDiff.gradient(f, pt)
x -> f(x...) + fx * (x[1]-pt[1]) + fy * (x[2]-pt[2])
end
```

It isn't so bad, but we need to specialize to the number of dimensions, use indexing,  and with more dimensions, it clearly would get tedious. Using vectors, we might have:

```
function tangent_plane(f, pt)
∇f = ForwardDiff.gradient(f, pt) # using a variable ∇f
x -> f(pt) + ∇f ⋅ (x - pt)
end
```

This is much more like the compact formula and able to handle higher dimensions without rewriting.



## Newton's method to find when $f(x,y)=0$ *and* $g(x,y)$.

Following [Strang](https://ocw.mit.edu/resources/res-18-001-calculus-online-textbook-spring-2005/textbook/MITRES_18_001_strang_13.pdf) Newton's method for a univariate function uses the idea that to find out when $f(x)$ crosses $0$, look at when the tangent line crosses zero. Let $f(x,y):R^2\rightarrow R$ and $g(x,y):R^2 \rightarrow R$ be two scalar functions. Then to find out where $f(x,y)=0$ we look at when the tangent plane crosses $0$, and similarly for $g$. These crossings will be lines, and the intersection of two (non-parallel) lines will be a point-- the next guess.

We have by linearization:

$$~
\begin{align}
f(x,y) &\approx f(x_n, y_n)  + \frac{\partial f}{\partial x}\Delta x + \frac{\partial f}{\partial y}\Delta y \\
g(x,y) &\approx g(x_n, y_n)  + \frac{\partial g}{\partial x}\Detla x + \frac{\partial g}{\partial y}\Delta y,
~$$
where $\Delta x = x- x_n$ and $\Delta y = y-y_n$. Setting $f(x,y)=0$ and $g(x,y)=0$, leaves these two linear equations in $\Delta x$ and $\Delta y$:

$$~
\begin{align}
\frac{\partial f}{\partial x} \Delta x + \frac{\partial f}{\partial y} \Delta y &= -f(x_n, y_n)\\
\frac{\partial g}{\partial x} \Delta x + \frac{\partial g}{\partial y} \Delta y &= -g(x_n, y_n).
\end{align}
~$$

We can use `Julia`'s `\\` operation to solve this, if we express in matrix form.


For example, consider the [bicylinder](https://blogs.scientificamerican.com/roots-of-unity/a-few-of-my-favorite-spaces-the-bicylinder/) the interesection of two perpendicular cylinders of the same radius. If the radius is $1$, we might express these by the functions:

$$~
f(x,y) = \sqrt{1 - y^2}, \quad g(x,y) = \sqrt{1 - x^2}.
~$$

We see that $(1,1)$, $(-1,1)$, $(1,-1)$ and $(-1,-1)$ are solutions to $f(x,y)=0$, $g(x,y)=0$ *and*
$(0,0)$ is a solution to $f(x,y)=1$ and $g(x,y)=1$. What about a level like $1/2$, say?


We can find a value where both are $1/2$ through:

```
function newton_step(f, g, xn, yn)
    x = [xn, yn]
    M = [ForwardDiff.gradient(f, x)'; ForwardDiff.gradient(g, x)']
    b = -[f(xn, yn), g(xn, yn)]
    Delta = M \ b
    x + Delta
end


function nm(f, g, x0, y0)

    x = [x0, y0]

    ctr = 0
    err = maximum(abs(u(x...)) for u in (f,g))

    while err > 1e-10 && ctr < 25
      x = newton_step(f, g, x...)
      @show ctr, x
      err = maximum(abs(u(x...)) for u in (f,g))
      ctr += 1
    end

    ctr >= 25 && error("Too many steps")

    x
end

```

Rather than work with $f(x,y) = c$ we solve $f(x,y)^2 = c^2$, as that will be avoid issues with the square root not being defined. Here is one way to solve:

```
c = 1/2
f(x,y) = 1 - y^2 - c^2
g(x,y) = (1 - x^2) - c^2
nm(f, g, 1/2, 1/3)
```

That $x=y$ is not so surprising, and in fact, this problem can more easily be solved analytically through $x^2 = y^2 = 1 - c^2$.






## Implicit differentiation

Consider now, Viviani's curve found from the intersection of a cylinder with a sphere. For concreteness, we discuss a cylinder $x^2 + (z-1)^2 = 1$ and a sphere $x^2 + y^2 + z^2 = 2^2$. The point $(x,y,z) = (0, 0, 2)$ will be a point on both,  with $z$ value $2$. The point $(0, 2, 0)$ will be a point on both with $z=0$. What about when $z= 1$? We don't have functions written as $f(x,y)  = z$, but rather function of $3$ variables, generically: $F(x,y,z)=c$. Can we still use something like Newton's method to solve this?

For univariate functions, we used implicit differentiation to find $dy/dx$ when we had $F(x,y) = c$ instead of direction $y=f(x)$, by *assuming* a functional representation and using the chain rule. Akin to expressing $F(x,y) = F(x, f(x))$, so $dF(x,f(x))/dx$ can proceed with $df(x)/dx$ using the chain rule.

Similarly, implicit differentiation can be applied for *partial* derivatives. In our example, we would have:

$$~
2x \frac{\partial x}{\partial x} + 2(z-1) \frac{\partial z}{\partial x} =
2x + 2(z-1) \frac{\partial z}{\partial x} = 0,\quad
0 + 2(z-1) \frac{\partial z}{\partial y} = 0,
~$$

and

$$~
2x + 2z  \frac{\partial z}{\partial x} = 0, \quad
2y + 2z  \frac{\partial z}{\partial y} = 0.
~$$

XXX




## Optimization

## Constrained optimization, LaGrange multipliers

## Taylor's theorem

Taylor's theorem for a univariate function states that if $f$ has $k+1$ derivatives in an open interval around $a$, $f^{(k)}$ is continuous between the colosed interval from $a$ to $x$ then:

$$~
f(x) = \sum_{j=0}^k \frac{f^{j}(a)}{j!} (x-a)^k + R_k(x),
~$$

where $R_k(x) = f^{k+1}(\xi)/(k+1)!(x-a)^{k+1}$ for some $\xi$ between $a$ and $x$.

This theorem can be generalized to scalar functions, but the notation can be cumbersome.
Following [Folland](https://sites.math.washington.edu/~folland/Math425/taylor2.pdf) we use *multi-index* notation. Suppose $f:R^n \rightarrow R$, and let $\alpha=(\alpha_1, \alpha_2, \dots, \alpha_n)$. Then define the following notation:

$$~
|\alpha| = \alpha_1 + \cdots + \alpha_n, \quad
\alpha! = \alpha_1!\alpha_2!\cdot\cdots\cdot\alpha_n!,\quad
\vec{x}^\alpha = x_1^{\alpha_1}x_2^{\alpha_2}\cdots x_n^{\alpha^n}, \quad
\partial^\alpha f = \partial_1^{\alpha_1}\partial_2^{\alpha_2}\cdots \partial_n^{\alpha_n} f =
\frac{\partial^{|\alpha|}f}{\partial x_1^{\alpha_1} \partial x_2^{\alpha_2} \cdots \partial x_n^{\alpha_n}}.
~$$

This notation makes many formulas from one dimension carry over to higher dimensions. For example, the binomial theorem says:

$$~
(a+b)^n = \sum_{k=0}^n \frac{n!}{k!(n-k)!}a^kb^{n-k},
~$$

and this becomes:

$$~
(x_1 + x_2 + \cdots + x_n)^n = \sum_{|\alpha|=k} \frac{k!}{\alpha!} \vec{x}^\alpha.
~$$

Taylor's theorem then becomes:

If $f: R^n \rightarrow R$ is sufficiently smooth ($C^{k+1}) on an open convex set $S$ about $\vec{a}$ then if $\vec{a}$ and $\vec{a}+\vec{h}$ are in $S$,
$$~
f(\vec{a} + \vec{h}) = \sum_{|\alpha| \leq k}\frac{\partial^\alpha f}(\vec{a})}{\alpha!}\vec{h}^\alpha + R_{\vec{a},k}(\vec{h}),
~$$
where $R_{\vec{a},k} = \sum_{|\alpha|=k+1}\partial^\alpha f(\vec{a} + c\vec{h})\frac{\vec{h}^\alpha}{\alpha!}$ for some $c$ in $(0,1)$.

##### Example

The elegant notation masks what can be complicated expressions. Consider the simple case $f:R^2 \rightarrow R$ and $k=2$. Then this says:

$$~
f(x + dx, y+dy) = f(x, y) + \frac{\partial f}{\partial x} dx + \frac{\partial f}{\partial y} dy
+ \frac{partial^2 f}{\partial x^2} \frac{dx^2}{2} +  2\frac{partial^2 f}{\partial x\partial y} \frac{dx dy}{2} +
+ \frac{partial^2 f}{\partial y^2} \frac{dy^2}{2} + R_{\langle x, y \rangle, k}(\langle dx, dy \rangle).
~$$

Using $\nabla$ and $H$ for the hessian, this can be expressed as:

$$~
f(x + dx, y + dy) = f(x,y) + \nabla{f} \cdot \langle dx, dy \rangle + \langle dx, dy \rangle^t H \langle dx, dy \rangle +R_{\langle x, y \rangle, k}(\langle dx, dy \rangle).
~$$

As for $R$, the full term involves terms for $\alpha = (3,0), (2,1), (1,2)$, and $(0,3)$. Using $\vec{a} = \langle x, y\rangle$ and $\vec{h}=\langle dx, dy\rangle$:

$$~
\frac{\partial^3 f(\vec{a}+c\vec{h}}{\partial x^3} \frac{dx^3}{3!}+
\frac{\partial^3 f(\vec{a}+c\vec{h}}{\partial x^2\partial y} \frac{dx^2 dy}{2!1!} +
\frac{\partial^3 f(\vec{a}+c\vec{h}}{\partial x\partial y^2} \frac{dxdy^2}{1!2!} +
\frac{\partial^3 f(\vec{a}+c\vec{h}}{\partial y^3} \frac{dy^3}{3!}.
~$$

The exact answer is usually not as useful as the bound: $|R| \leq M/(k+1)! \|\vec{h}\|^{k+1}$.


##### Example

Newton's method to find a minimum ...





## Newton's method



## Questions

###### Question

###### Question

###### Question

###### Question
